# Interpretable Machine Learning

[https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)

## Main Points

* Log EVERYTHING
  * (First, need to make sure its stupid easy to pull info out of logs)
* "Hundreds of other prediction systems and databases fed into the score, making it impossible to know what caused the drop in her score"
  * Signal Explainability is KEY
  * Explainability is avoided because it appears to impede progress, but what if explainability helped make our models better?
* "A machine of algorithm that explains its predictions will find more acceptance"

## [Chapter 2: Interpretability]('./2_INTERPRET.md)

* Contrasted explanations are more valuable than complete ones
  * Class B was predicted because of input value X, but if it had been Y we would have gotten Class B
* Just select a couple explanations that convey the point - not every last one
* Know the audience that is consuming the explanations
* Identify abnormalities and draw attention to them
* Focus on things that the audience already believes in - they will fight things they dont want to accept
* General explanations that cover lots of cases are good

## Vocab

* Algorithm = human instructed
* ML = machine learned it by itself
