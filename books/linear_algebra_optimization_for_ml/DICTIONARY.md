# Dictionary: Linear Algebra and Optimization


- `row vector`
  - 1xd
  - 1x2 = (12, 3)
  - 1x3 = (3, 0, 7)
- `column vector`
  - vectors are assumed to be column vectors, unless specified otherwise
  - nx1
  - 2x1:
    - 23
    - 7
  - 3x1:
    - 7
    - 8
    - 1
- `scalars`
  - individual numerical values typically drawn from the real domain in most ML applications
- `vectors`
  - arrays of numerical values (i.e. arrays of scalars)
  - each numerical value is referred to as a `coordinate`
  - The individual numerical values of the arrays are referred to as `entries`, `components`, or `dimensions`
  - the number of `components` is referred to as the vector `dimensionality`
- `matrices`
  - rectagular arrays of numerical values containing both rows and column
  - to access a value, must specify its row index and column index
  - d properties of n individuals
    - size is n x d
  - n rows of individuals
    - containing d properties
  - the value of the (i,j)th entry of the matrix is:
    - jth property of the ith individual
  - $A = [a_{ij}]_{n \times d}$
  - `square matrix`
    - when rows = columns
  - `rectangular matrix`
    - a non-square matrix
    - tall when more rows than columns
    - wide when more columns than rows
- `commutative`
  - the order of numbers does not matter
  - A + B = B + A
- `Euclidian norm` aka `squared norm`
  - The norm defines the vector length and is denoted by $\|.\|$
  - converts to a unit vector
- `Manhattan norm` or `L_{1}-norm`
  - this is basically just taking the sum of the absolute values
- `attributes` aka `variables`
  - correspond to dimensions in vector space
- `vector space`
  - ex. the infinite set of all possible cartesian coordinates in two dimensions in relation to the origin
- `Linear algebra`
  - can be viewed as a generalized form of the geometry of cartesian coordinates in d dimensions
- `orthogonal`
  - A pair of vectors is `orthogonal` if their dot product is 0 (and the angle between them is 90).
  - The vector \bar{0} is considered orthogonal to every vector.
- `orthonormal`
  - A set of vectors is `orthonormal` if each pair in the set is mutually orthogonal and the norm of each vector is 1.
- `coordinate transformation` or `projection`
  - compute a new set of coordinates with respect to a changed set of directions
- Matrix multiplication is NOT `commutative`, but it is
  - `associative` - A(BC) = (AB)C
  - `distributive` - A(B+C) = AB + AC, (B+C)A = BA + CA
  - takes the dot product of
    - a row from the first matrix
    - a column of the second matrix
  - $C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj} $
  - reads as: Each element in C_{ij} is the result of:
    - summing:
      - the products of the corresponding elements from:
        - i-th row of A
        - j-th column of B
      - for each index k, up to n number of (rows of A, columns of B)
  - also note:
    - A_{*k} denotes a column vector, specifically the kth column of A
    - B_{m*} denotes a row vector, specifically the mth row of B
    - c_{km} denotes the element in the k-th row and m-th column of C
- `symmetric matrix`
  - is a square matrix that is its own transpose
- `diagonal`
  - the set of entries for which the row and column indices are the same
- `identity matrix` or `I`
  - a square matrix that has
    - values of 1 in all entries along the diagonal
    - values of 0 for all non-diagonal entries
  - AI = IA = A
  - AI_{d} = I_{n}A = A
- `diagonal matrix`
  - when non-diagonal entries are all 0
  - but the diagonal entries are not 1
  - `rectangular diagonal matrix`
    - non square
- `triangular matrix`
  - `upper triangular matrix`
    - A square matrix is an `upper triangular matrix` if all entries (i,j) below its main diagonal (i.e., satisfying i > j) are zeros
  - `lower triangular`
    - A matrix is `lower triangular` if all entries (i,j) above its main diagonal (i.e., satisfying i<j) are zeros
  - `strictly triangular`
    - A matrix is said to be *strictly* triangular if its triangular *and* all its diagonal elements are zeros
    - Basically, all the diagonal elements are zero, but above or below the diagonal has values
- `sparse matrix`
  - when most of the entries have 0 values
- `nilpotent`
  - when a matrix satisfies $A^{k} = 0 $ for some integer k
- `inverse`
  - of a square matrix A is another square matrix A^{-1}
  - not all matrices have an inverse
  - $AA^{-1} = A^{-1}A = I  $
  - $(AB)^{-1} = B^{-1}A^{-1}$ - need to invert the order when bring outside of the parentheses
    - (ABC)^{-1} = C^{-1}B^{-1}A^{-1}
- `orthogonal matrix`
  - square matrix whose inverse is its transpose
  - $AA^{\top} = A^{\top}A = I  $
