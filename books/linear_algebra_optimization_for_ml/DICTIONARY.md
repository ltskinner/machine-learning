# Dictionary: Linear Algebra and Optimization

## Notation and assumptions for the book

- Assume have an $n \times d $ data matrix D
  - contains **n** examples of data
  - each rowcontains **d**-dimensional data points
  - a `dimension` or `attribute` is one of the **d** properties of a data point
  - a **column** of D contains this property for all data instances

## Definitions

- `row vector`
  - 1xd
  - 1x2 = (12, 3)
  - 1x3 = (3, 0, 7)
  - B_{m*} denotes a row vector, specifically the mth row of B
  - a_{i} = ith row of A
- `column vector`
  - vectors are assumed to be column vectors, unless specified otherwise
  - nx1
  - 2x1:
    - 23
    - 7
  - 3x1:
    - 7
    - 8
    - 1
  - A_{*k} denotes a column vector, specifically the kth column of A
  - a_{j} = jth column of A
- summing operator
  - \sum_{i} = shorthand for 'sum over the index i'
  - \sum_{i=1}^{m} = sum over the index i, starting at i = 1, up to index m (number of elements)
- `scalars`
  - individual numerical values typically drawn from the real domain in most ML applications
- `vectors`
  - arrays of numerical values (i.e. arrays of scalars)
  - each numerical value is referred to as a `coordinate`
  - The individual numerical values of the arrays are referred to as `entries`, `components`, or `dimensions`
  - the number of `components` is referred to as the vector `dimensionality`
  - dot products give you scalar projections
  - cross products give you a new vector
- `matrices`
  - rectagular arrays of numerical values containing both rows and column
  - to access a value, must specify its row index and column index
  - d properties of n individuals
    - size is n x d
  - n rows of individuals
    - containing d properties
  - the value of the (i,j)th entry of the matrix is:
    - jth property of the ith individual
  - $A = [a_{ij}]_{n \times d}$
  - `square matrix`
    - when rows = columns
  - `rectangular matrix`
    - a non-square matrix
    - tall when more rows than columns
    - wide when more columns than rows
- `commutative`
  - the order of numbers does not matter
  - `addition` *is commutative*
    - A + B = B + A
  - `multiplication` *is NOT commutative* (in general)
    - $A x B \neq B x A  $
- `idempotent`
  - two unit vector product (vv^{\top}) is idempotent
  - means, V^n = V - raising to power doesnt change the value
- `idempotent property` of `projection matrices`:
  - $P^2 = P = (QQ^{T})(QQ^{T}) = Q(Q^{T}Q)Q^{T} = QQ^{T} $ - note (Q^{T}Q) is an identity
- Matrix multiplication is NOT `commutative`, but it is
  - `associative` - A(BC) = (AB)C
  - `distributive` - A(B+C) = AB + AC, (B+C)A = BA + CA
- `Euclidian norm` aka `squared norm`
  - The norm defines the vector length and is denoted by $\|.\|$
  - converts to a unit vector
  - $\|\bar{x} \|^{2} = \bar{x} \cdot \bar{x} = \sum_{i=1}^{2} x_{i}^{2} $
- `Manhattan norm` or `L_{1}-norm`
  - this is basically just taking the sum of the absolute values
- `attributes` aka `variables`
  - correspond to dimensions in vector space
- `vector space`
  - ex. the infinite set of all possible cartesian coordinates in two dimensions in relation to the origin
  - an infinite `set` of vectors satisfying certain types of `set closure` properties under addition and scaling operations
- `Linear algebra`
  - can be viewed as a generalized form of the geometry of cartesian coordinates in d dimensions
- `orthogonal`
  - A pair of vectors is `orthogonal` if their dot product is 0 (and the angle between them is 90).
  - The vector \bar{0} is considered orthogonal to every vector.
  - dot product is simple
    - turn into col vectors
    - multiply across rows, sum to get the result
  - Q = A is a common notation for orthogonal matrices
    - where Q is an orthogonal matrix
- `orthonormal`
  - A set of vectors is `orthonormal` if each pair in the set is mutually orthogonal and the norm of each vector is 1.
- `coordinate transformation` or `projection`
  - compute a new set of coordinates with respect to a changed set of directions
  - takes the dot product of
    - a row from the first matrix
    - a column of the second matrix
  - $C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj} $
  - reads as: Each element in C_{ij} is the result of:
    - summing:
      - the products of the corresponding elements from:
        - i-th row of A
        - j-th column of B
      - for each index k, up to n number of (rows of A, columns of B)
  - also note:
    - A_{*k} denotes a column vector, specifically the kth column of A
    - B_{m*} denotes a row vector, specifically the mth row of B
    - c_{km} denotes the element in the k-th row and m-th column of C
- `symmetric matrix`
  - is a square matrix that is its own transpose
- `diagonal`
  - the set of entries for which the row and column indices are the same
- `identity matrix` or `I`
  - a square matrix that has
    - values of 1 in all entries along the diagonal
    - values of 0 for all non-diagonal entries
  - AI = IA = A
  - AI_{d} = I_{n}A = A
- `diagonal matrix`
  - when non-diagonal entries are all 0
  - but the diagonal entries are not 1
  - `rectangular diagonal matrix`
    - non square
- `triangular matrix`
  - `upper triangular matrix`
    - A square matrix is an `upper triangular matrix` if all entries (i,j) below its main diagonal (i.e., satisfying i > j) are zeros
  - `lower triangular`
    - A matrix is `lower triangular` if all entries (i,j) above its main diagonal (i.e., satisfying i<j) are zeros
  - `strictly triangular`
    - A matrix is said to be *strictly* triangular if its triangular *and* all its diagonal elements are zeros
    - Basically, all the diagonal elements are zero, but above or below the diagonal has values
- `sparse matrix`
  - when most of the entries have 0 values
- `nilpotent`
  - when a matrix satisfies $A^{k} = 0 $ for some integer k
- `inverse`
  - of a square matrix A is another square matrix A^{-1}
  - not all matrices have an inverse
  - $AA^{-1} = A^{-1}A = I  $
  - $(AB)^{-1} = B^{-1}A^{-1}$ - need to invert the order when bring outside of the parentheses
    - (ABC)^{-1} = C^{-1}B^{-1}A^{-1}
  - An nxn square matrix A has linearly independent columns/rows if and only if it is invertible
  - the inverse of an orthogonal matrix is its transpose
- `left-inverse` of matrix A
  - $\bar{x} = (A^T A)^{-1} A^T  \bar{b} $
  - $L = (A^{T}A)^{-1}A^{T} $
  - $LA = (A^{T}A)^{-1}(A^{T}A) = I_d $
- `right-inverse` of matrix A
  - $\bar{x} = A^{T}(AA^{T})^{-1} \bar{b} $
  - $R = A^T(AA^T)^{-1} $
  - $AR = (AA^{T})(AA^{T})^{-1} = I_n  $
- `singular`
  - a matrix that is **not invertible**
  - non-square matrices
  - row of zeros, col of zeros
  - determinant is zero
- `orthogonal matrix`
  - square matrix whose inverse is its transpose
  - $AA^{\top} = A^{\top}A = I  $
  - the `product of orthogonal matrices`
    - not necessarily commutative in higher dimensions, so assume is NOT commutative
    - orthogonal matrices that do not correspond to the same `axis of rotation` may not be commutative
- `Push-Through Identity`
  - helpful when you want to "push" the *inverses* of A and C outside a product that includes B:
  - $A^{-1}BC^{-1} = (AC)^{-1}(ABC)  $
  - $(I + AB)^{-1}A = A(I + BA)^{-1}  $
  - $(I + AB)^{-1} = I - AB(I + AB)^{-1} = I - A(I + BA)^{-1}B  $
- `Parallelogram law`
  - States that the sum of the squares of the sides of a parallelogram is equal to the sum of the squares of its diagonals
- `Apollonius's Theorum / Apollonius Identity`
  - AB^{2} + AC^{2} = 2(AD^{2} + BD^{2})
  - aka
  - AB^{2} + AC^{2} = 2AD^{2} + 2BD^{2}
- `Sine Law`
  - soh cah toa
  - sin(\theta) = \frac{\|A\crossB\|}{\|a\|\|b\|}
  - cos(\theta) = \frac{\|A\cdot\|}{\|a\|\|b\|}
  - tan(\theta) = \frac{\|A\crossB\|}{\|A\cdot\|}
  - sin^{2}(x) + cos^{2}(x) = 1
  - \frac{\|a\|}{sin(A)} = \frac{\|b\|}{sin(B)}
- `trigonometric identity` (one of them)
  - cos(\theta_{1} \theta_{2}) = cos(\theta_{1}cos(\theta_{2})) - sin(\theta_{1})sin(\theta_{2})
- `Neumann Series Approximation`
  - $(I + X)^{-1} \approx I - X + X^{2} - X^{3} + X^{4}  $
  - $(I + AB)^{-1} \approx I - AB + (AB)^{2} - (AB)^{3} + (AB)^{4}  $
- `Frobenius norm`
  - $\|A\|_{F} = \|A^{\top}\|_{F} = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{d} a_{ij}^{2}  }  $
  - The squared Frobenius norm is the sum of squares of the norms of the row-vectors (or, alternatively, column vectors) in the matrix
    - THE POINT OF THIS IS TO GET RID OF SQRT
- `energy`
  - another name for the squared Frobenius norm
  - The energy of a rectangular matrix A is equal to the trace of either AA^{\top} or A^{\top}A
    - $\|A\|_{F}^{2} = Energy(A) = tr(AA^{\top}) = tr(A^{\top}A)  $
- `trace`
  - tr(A) of a square matrix is defined by the sum of its diagonal entries
  - tr(A) = \sum_{i=1}^{n} a_{ii}^{2}
- `elementary row operations`
  - pre-multiplied = OX
  - `interchange operation`
    - the ith and jth rows of the matrix are interchanged
    - the operation is fully defined by two indices i and j in any order
    - the inverse is the transpose (typically, the same matrix)
  - `addition operation`
    - a scalar multiple of the jth rows is added to the ith row
    - this operation is defined by two indices i,j in a specific order, and a scalar multiple c
    - inverse is -(c), whatever the c addition factor is not on the diagonal
  - `scaling operation`
    - the ith row is multiplied with scalar c
    - the operation is fully defined by the row index i and the scalar c
    - inverse is 1/c, whatever the c scaling factor is on the diagonal
- `elementary column operations`
  - post-multiplied = XO
  - exactly analogous operations to the row operations
- `elementary matrix`
  - differs from the identity matrix by applying a single row or column operation
  - These types of elementary matrices are always invertible
  - The inverse of the interchange matrix is itself
  - applying to `non-square matrices`
    - For an *n x d* matrix:
      - the pre-multiplication operator matrix will be of size *n x n*
      - the post-multiplication operator matrix will be of size *d x d*
  - there are also `geometric` elementary matrices
    - `rotation operation`
      - swapping between pre- and post-multiplication, need to Transpose
    - `reflection operation`
      - same between pre- and post-
      - is a special case of a scaling (diagonal) matrix (by negative values)
    - `scaling operation`
      - same between pre- and post-
      - aka:
        `dilation`
        `contraction`
      - `anisotropic` - when scaling factors across different dimensions are different
- `permutation matrix`
  - contains a single 1 in each row, and a single 1 in each column
  - elementary interchange matrices are a special case of the permutation matrix
  - pre-multiplying shuffles the rows
  - post-multiplying shuffles the columns
  - permutation matrix and its transpose are inverses of one another
    - this differs from pure row or pure col operations
    - well, or the transpose of pure row or pure col is identical as well lol
- `matrix polynomial`
  - come in the form of:
    - f(U) = \sum_{i=0}^{k}c_{i}A^{i} = c_{0}I + c_{1}U + c_{2}U^{2} + .. + c_{k}U^{k}
- `Givens Rotations`
  - of form G(plane_1, plane_2, angle)
  - take standard rotation matrix, and overlay on top of identity matrix
  - where rows and columns correspond to plane_1 and plane_2, with cos and sin operators
  - note, row and col G_{r|c} matrices are inverts of each other
  - there is usually an elementary reflection included as well
- `Householder reflection matrix`
  - an orthogonal matrix that reflects a vector \bar{x} into any "mirror" hyperplane of arbitrary orientation
  - $(I - 2\bar{v}\bar{v}^{\top})$ is an elementary reflection matrix in the hyperplane perpendicular to \bar{v} and passing through the origin.
  - Householder is a more expressive transformation than Givens because orthogonal matrix can be represented with fewer reflection matrices
- `rigid` transformations
  - transformations that preserve distances between pairs of points
- `singular value decomposition`
  - states that any square matrix A can be expressed in the form $A = U\Delta V^{\top}  $, where U and V are both orthogonal matrices (which might be different) and $\Delta$ is a *nonnegative* scaling matrix
  - Therefore, *all linear transformations defined by matrix multiplication can be expressed as a sequence of rotations/reflections, together with a single ansiotripic scaling*

## Linear Transformations

- `Linear Transform`
  - `translation` is NOT allowed
  - A vector-to-vector function f(\bar{x}) defines a linear transformation of \bar{x}
  - A vector-to-vector fn is a generalization of the notion of scalar functions, and it maps a d-dimensional vector to an n-dimensional vector for some d and n
  - f(x) = Ax is one because accepts a vector, and then returns a vector
  - all `linear transforms` are special cases of `affine transforms`, but **NOT vice versa**
  - The class of linear transforms can always be geometrically expressed as a sequence of one or more rotations, reflections, dilations/contractions about the origin. The origin always maps to itself after these operations, and therefore translation is not included
- `Linear Function`
  - `translation` IS allowed
- `vector spaces`
  - the collection of row vectors and column vectors into data matrices
- `vector subspace`
  - A vector space S is a subspace of another vector space V, if any vector x \in S is also present in V
    - $S \subseteq V$ is a subspace
  - the subspace S is a `proper subspace` of V when V contains vectors not present in S
    - $S \subset V $ is a proper subspace
- `disjoint vector spaces`
  - two vector spaces are disjoint if and only if the two spaces do not contain any vector in common other than the zero vector
- `orthogonal vector spaces`
  - if and only if the dot product between any two vectors of each U and W is zero, then the spaces are orthogonal
  - Disjoint pairs of vector spaces need not be orthogonal, but orthogonal pairs of vector spaces are always disjoint
    - you can be disjoint without being orthogonal
    - but if you are orthogonal, you must be disjoint
- `orthogonal complementary subspace`
  - there are many complementary subspaces
  - but only one *orthogonal* complementary subspace
  - the union of the basis sets of each subspace form the new basis for R^n
  - basically, an orthogonal (perpendicular) vector exiting a 2d subspace
- `linearly dependent`
  - A set of non-zero vectors $\{\bar{a}_{1}...\bar{a}_{d} \}  $ is linearly dependent, if a set of d scalars $x_{1}...x_{d}  $ can be found so that at least some of the scalars are non-zero, and the following condition is satisfied
  - $\sum_{i=1}^{d} x_{i}\bar{a}_{i} = \bar{0}  $
  - `non-trivial` if every scalar in $x_{1}...x_{d}  $ is **NOT zero**
  - `trivial` if there **ARE** zeros
  - when you create new vectors or points from two basis vectors, these new points are linearly dependent on the basis vectors
  - new points are not directly linearly dependent on each other tho
- `linearly independent`
  - when no set of non-zero scalars can be found
  - "contributes a new, independent direction"
  - if a matrix is invertible, it has linearly independent columns/rows
- `basis` aka `basis set`
  - basically like the fundamental axes by which you define a space
  - for any vector v \in V,, we can find scalars x_1...x_d so that $\bar{v} = \sum_{i=1}^{d} x_{i}\bar{a}_{i}  $
  - one cannot do this for any proper subset of B
  - *The vectors in a basis must be linearly independent*
  - is a **maximal** set of `linearly independent` vectors in it
  - the coordinates of a vector in any basis must be unique
  - `standard basis`
    - is like an `identity matrix`
    - the identity matrix is a collection of the `standard basis` *vectors*, arranged in matrix form
    - cannot be used the basis of a `proper` subspace
- `dimensionality`
  - The number of members in every possible basis set of a vector space V is always the same. This value is referred to as the **dimensionality** of the vector space
- `span`
  - the vector space defined by all possible linear combinations of the vectors in a set
- `translation`
  - the `translation` operator is NOT a linear transform
  - however, is used for `mean-centering` the data
  - special case of `affine transformations`
- `mean-centering`
  - a constant mean vector is subtracted from each row of the dataset
  - this way, the mean value of each column is transformed to 0
- `affine transformations`
  - a combination of linear transform with a translation
  - includes any transform of the form:
    - f(x) = Ax + c
      - where both x and c are vectors
      - A is an nxd matrix
      - x is a d-dimensional vector
      - c is an n-dimensional column vector
- `rotreflection`
  - In some cases, reflections are included with rotations
  - When a compulsory reflection is included in the sequence, the resulting matrix is referred to as a `rotreflection` matrix
- `column space` aka `column rank`
  - defined as the vector space spanned by the columns of nxd matrix A, and is a subspace of **R^n**
- `row space` aka `row rank`
  - defined as the vector space spanned by the columns of A^T, and is a subspace of **R^d**
  - the dimensionality of the row rank and column rank of any nxd matrix A **is the same**
  - given a matrix, remove all linearly dependent rows
- `matrix rank`
  - the rank of a matrix is equal to the rank of its row space, which is the same as the rank of its column space
- `full rank`
  - `positive-definite` full rank square matrices have an empty null space
    - full rank matrices must be `invertible`
  - where the rows of a *square matrix* must be linearly independent when the columns are linearly independent, these matrices are of `full rank`
  - for *rectangular matrices* to be full rank, *either* the rows or columns are linearly independent
    - `full row rank`
      - when rows are linearly independent
    - `full column rank`
      - when columns are linearly independent
- `null space`
  - The null space of a matrix A is the subspace of R^d (cols) containing all column vectors $\bar{x} \in R^d  $, such that $A\bar{x} = \bar{0} $
  - the **orthogonal complementary subspace of the row space of A**
  - for square and non-singular matrices, the null space only contains the zero vector
  - `right null space`
    - the default null space because the vector x occurs on the right side of matrix A in the product Ax, which must eval to the zero vector
    - the orthogonal complementary subspace of the `row space` of A. defined by the (transposed) rows of A
    - set the row space = [0, 0, 0, 0] and solve
      - i want to say this is because the product of the basis produces an orthogonal vector, and something about the zero vector is related to orthogonality. yeah, two vectors or matrices that are orthogonal have a product of zero
    - Consider an nxd matrix A with rank k <= min(n,d). The rank of the `null space` of A is d-k
  - `left null space`
    - the orthogonal complement of the vector space spanned by the columns of the matrix
    - the orthogonal complementary subspace of the `column space` of A
    - set the column space = [0, 0, 0, 0] and solve
    - Consider an nxd matrix A with rank k <= min(n,d). The rank of the `left null space` of A is (n-k)
- `four fundamental subspaces of linear algebra`
  - the row space
  - the column space
  - the right null space
  - the left null space
- `strictly positive` matrix
  - only positive and non-zero values
- `positive definite` matrix
  - only positive and zero values
- `Gram matrix`
  - $A^T A$
  - the Gram matrix of the `column space` of A
  - the columns of A are `linearly independent` if the Gram matrix is invertible
- `left Gram matrix`
  - $ AA^T $
  - the Gram matrix of the `row space`
- `inconsistent`
  - a matrix with a row of zeros at the end
  - have no solution bc a zero value on the left is equated with a non-zero value on the right

## ML specifics

### Problem Frames

- `matrix factorization` is an alternative term for `matrix decomposition`
  - typically refers to an **optimization centric** view of decomposition
- `clustering`
  - partiioning the rows of the nxd data matrix D into groups of similar rows
- `classification`
  - closely related to clustering, except that more guidance is available for grouping the data with the use of the notion of *supervision*
  - in addition to the nxd data matrix D, we have an nx1 array of labels denoted by \bar{y}
  - labels are `categorical`
- `regression`
  - identical to classification, except:
  - labels are `numerical`
  - `dependent` variable aka:
    - `response variable`
    - `target variable`
    - `regressand`
  - `independent` variables aka `regressors`
- `outlier detection`
  - we would like to find rows of D that are very different from most of the other rows
  - this has a complementary relationship with the clustering problem

### Misc

- `gradient`
  - the d-dimensinoal vector of partial derviatives
- `gradient descent`
  - use a computaiton algorithm of initializing the parameter set \bar{W} randomly (or a heuristically chosen point)
  - then change the parameter set in the direction of the negative derivative of the objective fn
- `learning rate`
  - the rate at which the parameters are changed
  - the step size is $\alpha$
  - Since the gradients change on making a step, one must be careful not to make steps that are too large or else the effects may be unpredictable
- `gradient vector`
  - defines an instantaneous direction of best rate of improvement of the objective fn at the current value of the parameter vector
- `convergence`
  - steps are repeatedly executed until further improvements become too small to be useful
  - such a situation will occur when the gradient vector contains near-zero entries
