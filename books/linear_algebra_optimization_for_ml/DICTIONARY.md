# Dictionary: Linear Algebra and Optimization


- `row vector`
  - 1xd
  - 1x2 = (12, 3)
  - 1x3 = (3, 0, 7)
  - B_{m*} denotes a row vector, specifically the mth row of B
- `column vector`
  - vectors are assumed to be column vectors, unless specified otherwise
  - nx1
  - 2x1:
    - 23
    - 7
  - 3x1:
    - 7
    - 8
    - 1
  - A_{*k} denotes a column vector, specifically the kth column of A
- `scalars`
  - individual numerical values typically drawn from the real domain in most ML applications
- `vectors`
  - arrays of numerical values (i.e. arrays of scalars)
  - each numerical value is referred to as a `coordinate`
  - The individual numerical values of the arrays are referred to as `entries`, `components`, or `dimensions`
  - the number of `components` is referred to as the vector `dimensionality`
- `matrices`
  - rectagular arrays of numerical values containing both rows and column
  - to access a value, must specify its row index and column index
  - d properties of n individuals
    - size is n x d
  - n rows of individuals
    - containing d properties
  - the value of the (i,j)th entry of the matrix is:
    - jth property of the ith individual
  - $A = [a_{ij}]_{n \times d}$
  - `square matrix`
    - when rows = columns
  - `rectangular matrix`
    - a non-square matrix
    - tall when more rows than columns
    - wide when more columns than rows
- `commutative`
  - the order of numbers does not matter
  - `addition` *is commutative*
    - A + B = B + A
  - `multiplication` *is NOT commutative* (in general)
    - $A \times B = \neq B \times A  $
- `Euclidian norm` aka `squared norm`
  - The norm defines the vector length and is denoted by $\|.\|$
  - converts to a unit vector
  - $\|\bar{x} \|^{2} = \bar{x} \cdot \bar{x} = \sum_{i=1}^{2} x_{i}^{2} $
- `Manhattan norm` or `L_{1}-norm`
  - this is basically just taking the sum of the absolute values
- `attributes` aka `variables`
  - correspond to dimensions in vector space
- `vector space`
  - ex. the infinite set of all possible cartesian coordinates in two dimensions in relation to the origin
- `Linear algebra`
  - can be viewed as a generalized form of the geometry of cartesian coordinates in d dimensions
- `orthogonal`
  - A pair of vectors is `orthogonal` if their dot product is 0 (and the angle between them is 90).
  - The vector \bar{0} is considered orthogonal to every vector.
- `orthonormal`
  - A set of vectors is `orthonormal` if each pair in the set is mutually orthogonal and the norm of each vector is 1.
- `coordinate transformation` or `projection`
  - compute a new set of coordinates with respect to a changed set of directions
- Matrix multiplication is NOT `commutative`, but it is
  - `associative` - A(BC) = (AB)C
  - `distributive` - A(B+C) = AB + AC, (B+C)A = BA + CA
  - takes the dot product of
    - a row from the first matrix
    - a column of the second matrix
  - $C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj} $
  - reads as: Each element in C_{ij} is the result of:
    - summing:
      - the products of the corresponding elements from:
        - i-th row of A
        - j-th column of B
      - for each index k, up to n number of (rows of A, columns of B)
  - also note:
    - A_{*k} denotes a column vector, specifically the kth column of A
    - B_{m*} denotes a row vector, specifically the mth row of B
    - c_{km} denotes the element in the k-th row and m-th column of C
- `symmetric matrix`
  - is a square matrix that is its own transpose
- `diagonal`
  - the set of entries for which the row and column indices are the same
- `identity matrix` or `I`
  - a square matrix that has
    - values of 1 in all entries along the diagonal
    - values of 0 for all non-diagonal entries
  - AI = IA = A
  - AI_{d} = I_{n}A = A
- `diagonal matrix`
  - when non-diagonal entries are all 0
  - but the diagonal entries are not 1
  - `rectangular diagonal matrix`
    - non square
- `triangular matrix`
  - `upper triangular matrix`
    - A square matrix is an `upper triangular matrix` if all entries (i,j) below its main diagonal (i.e., satisfying i > j) are zeros
  - `lower triangular`
    - A matrix is `lower triangular` if all entries (i,j) above its main diagonal (i.e., satisfying i<j) are zeros
  - `strictly triangular`
    - A matrix is said to be *strictly* triangular if its triangular *and* all its diagonal elements are zeros
    - Basically, all the diagonal elements are zero, but above or below the diagonal has values
- `sparse matrix`
  - when most of the entries have 0 values
- `nilpotent`
  - when a matrix satisfies $A^{k} = 0 $ for some integer k
- `inverse`
  - of a square matrix A is another square matrix A^{-1}
  - not all matrices have an inverse
  - $AA^{-1} = A^{-1}A = I  $
  - $(AB)^{-1} = B^{-1}A^{-1}$ - need to invert the order when bring outside of the parentheses
    - (ABC)^{-1} = C^{-1}B^{-1}A^{-1}
- `orthogonal matrix`
  - square matrix whose inverse is its transpose
  - $AA^{\top} = A^{\top}A = I  $
- `Push-Through Identity`
  - helpful when you want to "push" the *inverses* of A and C outside a product that includes B:
  - $A^{-1}BC^{-1} = (AC)^{-1}(ABC)  $
  - $(I + AB)^{-1}A = A(I + BA)^{-1}  $
  - $(I + AB)^{-1} = I - AB(I + AB)^{-1} = I - A(I + BA)^{-1}B  $
- `Frobenius norm`
  - $\|A\|_{F} = \|A^{\top}\|_{F} = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{d} a_{ij}^{2}  }  $
  - The squared Frobenius norm is the sum of squares of the norms of the row-vectors (or, alternatively, column vectors) in the matrix
- `energy`
  - another name for the squared Frobenius norm
  - The energy of a rectangular matrix A is equal to the trace of either AA^{\top} or A^{\top}A
    - $\|A\|_{F}^{2} = Energy(A) = tr(AA^{\top}) = tr(A^{\top}A)  $
- `trace`
  - tr(A) of a square matrix is defined by the sum of its diagonal entries
- `elementary row operations`
  - pre-multiplied = OX
  - `interchange operation`
    - the ith and jth rows of the matrix are interchanged
    - the operation is fully defined by two indices i and j in any order
    - the inverse is the transpose (typically, the same matrix)
  - `addition operation`
    - a scalar multiple of the jth rows is added to the ith row
    - this operation is defined by two indices i,j in a specific order, and a scalar multiple c
    - inverse is -(c), whatever the c addition factor is not on the diagonal
  - `scaling operation`
    - the ith row is multiplied with scalar c
    - the operation is fully defined by the row index i and the scalar c
    - inverse is 1/c, whatever the c scaling factor is on the diagonal
- `elementary column operations`
  - post-multiplied = XO
  - exactly analogous operations to the row operations
- `elementary matrix`
  - differs from the identity matrix by applying a single row or column operation
  - These types of elementary matrices are always invertible
  - The inverse of the interchange matrix is itself
  - applying to `non-square matrices`
    - For an *n x d* matrix:
      - the pre-multiplication operator matrix will be of size *n x n*
      - the post-multiplication operator matrix will be of size *d x d*
- `permutation matrix`
  - contains a single 1 in each row, and a single 1 in each column
  - elementary interchange matrices are a special case of the permutation matrix
  - pre-multiplying shuffles the rows
  - post-multiplying shuffles the columns
  - permutation matrix and its transpose are inverses of one another
    - this differs from pure row or pure col operations
    - well, or the transpose of pure row or pure col is identical as well lol
