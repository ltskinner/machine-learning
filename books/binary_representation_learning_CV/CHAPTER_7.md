# Chapter 7. Semantic-Aware Adversarial Training

Recent investigation into adversarial deep hashing networks have underscored the security threat posed by adversarial examples, known as adversarial vulnerability. Effectively distilling reliable semantic representatives for deep hashing to guide adversarial learning proves challenging, impeding the improvement of adversarial robustness in deep hashing-based retrieval models. Additionally, existing researhc on adversarial training for deep hashing lacks a unified minimax strucutre. This chapter introduces the Semantic-Aware Adversarial Training (SAAT) to enhance the adversarial robustness of deep hashing models. A discriminative mainstay features learning (DMFL) scheme is concieved to construct semantic representatives for guiding adversarial learning in deep hashing. DMFL, with a strict theoretical guarantee, is adaptively optimized in a discriminative learning manner, considering both discriminative and semantic properties jointly. Adversarial examples are generated by maximizing the Hamming distance between hash codes of adversarial samples and mainstay features, validated for efficacy inadversarial attack trials. Notably, this chapter formulates the formalized adversarial training of deep hashing into a unified minimax optimization for the first time, guided by generated mainstay codes. Extensive experiments on benchmark datasets demonstrate superb attack performance against sota algorithms, while the proposed adversarial training effectively eliminates adversarial perturbations, ensuring trustworthy deep hashing-based retrieval

## 7.1 Introduction

Recent studies have shown that deep hashing-based retrieval models are vulnerable to adversarial examples.

In deep hashing-based face recognition systems, adversarial samples can mislead the system.

Current research working with adversarial examples mostly focus on classification task, with some works studying adversarial learning on deep hashing-based retrieval. These are very different. the first learns to classify each sasmple to its target class under supervision of class labels. The latter is a similarity preserving task

It is clear that we can generate adversarial examples that can decieve and out trick the classification system by minimally recomposing classification boundaries derived from semantic labels. However, for the semantic similarity-preserving hashing-based retrieval task, there are no explicit representatives (e.g. label) to lead adversarial attacks or defenses, resulting in more challenges compared to the classification one. Moreover, the minimax optimization based adversarial training, which has shown success in classification, does not seamlessly transfer to deep hashing due to the absence of semantic representations. Challenging to address these two problems:

1. How could we initialize the optimal and global semantic representatives for adversarial learning in deep semantic preserving hashing networks?
2. How could we derive a unified minimax formulation for deep hashing to conduct robust adversarial training?

For regular classification, an adversarial attack with the guidance of labels only needs to make the adversrail samples cross the decision boundary

For hashing, way more difficult because the adversarial samples are expected to blend into the clusters in the embedding space. Deep hashing needs reliable and discriminative

For the first question, some adversarial attack works heuristically select the hash code of a single sample as each representative for generating its adversarial example, while others employ a set of hash codes from multiple relevant samples. Nevertheless, these methods are not reliable and fail to capture globally discriminative semantics, yielding low-quality semantic representatives. Another line of work adopts an auxiliary network to learn representative hash codes of label encoding for targetted attacks. Typically, such an auxiliary prototype network is a parameter-sensitive scheme that lacks theoretical guarantees, so the whole adversrial generation is poorly generalized over different datasets and hashing networks

For the second question, there is only one work that simply reduces the distance between adversarial examples and the benign samples in Hamming space. However, this method is hard to enable robust adversarial training because it is not a standard minimax adversarial optimization probelm, composed of an *inner maximization* problem and an *outer minimization* problem. Notably, such a unified learning framework cannot hold in regular deep hashing-based retreival networks unless prefereable semantic representatives are provided.

To overcome above issues, mainly constructs discriminative semantic representatives (called `mainstay codes`) for adversarial learning of deep hashing and further proposes Semantic-Aware Adversarial Training (SAAT) with a formalized minimax framework to strengthen the adversarial robustness. Intuitively, we rethink the charactersitics of semantic similarity-preserving hashing-based retrieval tasks. Different from classification, the purpose of retrieval is to return top-n relevant objects instead of one result. Hence, the optimal semantic representative of given sample x in the retrieval task should preserve both similarities with all semantically relevant samples (positives) and dissimilarity with all semantically irrelevant ones (negatives). From this viewpoint, we argue that the semantic representative for adversarial deep hashing is expected to have a minimum Hamming distance from all positives yet a maximum distance from all negatives.

We allow an efficient approach called Discriminative Mainstay Features Learning (DMFL), whic offers rogorous theoretical guarantees and enables the direct acquisition of the optimal solution to the problem, i.e., the proposed mainstay code. Then, we transform the adversarial attack (e.g. non-targetted attack) on deep hashing into maximizing the Hamming distance between the hash code of the adversarial example and the mainstay code to efficiently generate the optimal adversarial example. Furthermore, based on the generated mainstay codes, we formulate the adversarial training ond eep hashing as a minimax optimization problem, i.e. a unified and standard adversraial training formula. Under the minimax paradigm, the inner maximization seeks an adversarial example x' of a given data x whose hash code maximizes the Hamming distance from the mainstay code, and the outer minimzation attempts to find model parameters so that hash code of x' is close to the mainstay code to alleviate the effects caused by the adversarial perturbations

Main contributions:

- Semantic-Aware Adversarial Training (SAAT)
  - first attempt to formulate the formalized adversarial training of deep hashing into a unified minimax paradigm guided by well-designed globally optimal semantic representatives
- Discriminative mainstay features learning (DMFL) scheme
  - generates global semantic representatives (mainstay codes)
- training strategy under a minimax optimization

## 7.2 Semantic-Aware Adversarial Training

### 7.2.1 Preliminaries

#### Deep Hashing-Based Retrieval

#### Definition 7.1 - Adversarial Attach on Deep Hashing-Based Retrieval

The goal of non-targetted attack is to craft an adversarial sample x' which could confuse the deep hashing model F to retrieve samples irrelevant to query x. In constrast, a targetted attack aims to mislead the deep heashing model into retruning samples related to given target label y_{t}. Moreover, the adversarial perturbation x' - x should be as small as possible to be imperceptible to the human eye

#### Definition 7.2 - Adversarial Training on Deep Hashing-Based Retrieval

similar to classification, adversarial training on deep hashing utilizes both the benign samples and corresponding adversarial versions to reoptimize the parameter \theta of depe hashing model, and thereby the model could retrieve semantically relevant contents to the original label y_{i}, whether the input is a clean sample x_{i} or an adversarial sample x_{i}'

### 7.2.2 An Overall Illustration

Deep hashing suffers from a lack of discriminative representatives to guide adversarial attack and defense. The minastay code which is semantic-aware is created

Under formalized adversarial training, the inner maximization aims to generate adversarial examples led by the mainstay codes, while the outer minimization attemps to learn robust models by minimizing the expected loss over the adversarial perturbations

### 7.2.3 Generation of Mainstay Code

here, a global semantic space means that the method considers all positive and negative samples related to the query in the global semantic space

### 7.2.4 Semantic-Aware Adversarial Attack

#### Non-targetted Attack

Here, prefer maximizing the hash code distance between the adversarial example and its semantically relevant samples and simultaneously minimizing the distance from irrelevant samples, rather than only the benign sample

#### Targetted Attack

Only difference between non-targetted and targetted is the objective fn

#### Generation of Adversarial Examples

### 7.2.5 Semantic-Aware Adversarial Training

## 7.3 Experiments

### 7.3.1 Experimental Setup

#### 7.3.1.1 Datasets

#### 7.3.1.2 Baselines

#### 7.3.1.3 Implementation Details

#### 7.3.1.4 Protocols

Mean avg precision

### 7.3.2 Adversarial Attack Results

### 7.3.3 Adversarial Defense Results

### 7.3.4 Analysis and Discusisons

#### 7.3.4.1 Attack Results in Theory

#### 7.3.4.2 Effect of T in PGD

#### 7.3.4.3 Analysis on Hyper-parameters

#### 7.3.4.4 Perceptibility

#### 7.3.4.5 Universality on Different Hashing Models

## 7.4 Conclusion

too bad this book ended with adversarial crap, otherwise really appreciated the work here
